#!/usr/bin/env python3

"""
This script orchestrates all testing and debugging activities for the user authentication system.
It ensures:
- Comprehensive end-to-end functionality tests
- Security vulnerability scanning and assessment
- Edge case testing for boundary conditions
- System diagnostics and health checks
- Integration with existing test scripts

In the function run_server_check , if the port or host used is different from the default one, 
the URL and host should be adjusted accordingly.
"""

import json
import sys
import time
from datetime import datetime
from typing import Dict, Any, List, Optional, cast

class MasterTestSuite:
    """
    This class coordinates all testing activities and provides unified reporting.
    It manages the execution of different test suites and aggregates their results
    into a comprehensive final report for system validation.
    """
    
    def __init__(self) -> None:
        """
        Initialize the Master Test Suite to set up the environment and prepare the test results.
        """
        self.start_time: float = time.time()
        
        # Initialize result tracking with proper type annotations
        # Each test category can store detailed results or None if not run
        self.test_results: Dict[str, Optional[Dict[str, Any]]] = {
            'comprehensive_tests': None,    # End-to-end functionality tests
            'security_scan': None,          # Security vulnerability assessment
            'edge_case_tests': None,        # Boundary condition testing
            'system_debug': None            # System diagnostics and health checks
        }
        
    def run_server_check(self) -> bool:
        """
        Check if the Flask server is running and provide the clear status of the server and also ensures that the test should run
        only if the server is running. 
        """
        print("üîå Checking if Flask server is running...")
        
        try:
            import requests  # Import requests module for HTTP communication
            # Attempt to connect to the Flask development server
            # Timeout of 5 seconds prevents hanging on unresponsive servers
            response = requests.get("http://127.0.0.1:5000", timeout=5) # Note: If the server is running on a different port or host, adjust the URL accordingly
            
            if response.status_code == 200:
                print("‚úÖ Flask server is running and accessible")
                return True
            else:
                print(f"‚ö†Ô∏è Flask server responded with status {response.status_code}")
                return False
                
        except Exception as e:
            print(f"‚ùå Flask server is not accessible: {str(e)}")
            print("\nüí° To start the server, run: python app.py")
            return False

    def run_comprehensive_tests(self) -> bool:
        """
        This method executes the main comprehensive test suite that validates
        all core functionality of the authentication system including:
        - User registration and validation
        - Login and authentication flows
        - Session management and token handling
        - Admin functionality and permissions
        - API endpoint responses and error handling
        """
        print("\n" + "="*60)
        print("üöÄ RUNNING COMPREHENSIVE END-TO-END TESTS")
        print("="*60)
        
        try:
            # Import and initialize the comprehensive test suite
            from comprehensive_test_suite import ComprehensiveTestSuite
            
            tester = ComprehensiveTestSuite()
            tester.run_all_tests()
            
            # Extract and analyze test results with proper type handling
            passed: int = len(tester.passed_tests)
            failed: int = len(tester.failed_tests)
            total: int = len(tester.test_results)
            
            # Store results with explicit type annotations to prevent type errors
            self.test_results['comprehensive_tests'] = {
                'passed': passed,
                'failed': failed,
                'total': total,
                'success_rate': (passed/total*100) if total > 0 else 0.0,
                'status': 'PASS' if failed == 0 else 'FAIL'
            }
            
            print(f"\nüìä Comprehensive Tests Summary: {passed}/{total} passed")
            return failed == 0
            
        except Exception as e:
            print(f"‚ùå Error running comprehensive tests: {str(e)}")
            # Store error information with proper typing
            self.test_results['comprehensive_tests'] = {
                'status': 'ERROR', 
                'error': str(e),
                'passed': 0,
                'failed': 0,
                'total': 0,
                'success_rate': 0.0
            }
            return False

    def run_security_scan(self) -> bool:
        """
        Run the security vulnerability scanner which scans for defined Security Assessment Areas like:
        - Authentication mechanism strength and implementation
        - Session management and token security
        - Input validation and injection prevention
        - Access control and authorization mechanisms
        - Cryptographic implementation and key management
        - Rate limiting and brute force protection
        - Information disclosure vulnerabilities
        - OWASP Top 10 compliance check

        Note: the score of 70 or below is considered a failure, as it may have many vulnerabilities that may need to be fixed immediately.
        """
        print("\n" + "="*60)
        print("üõ°Ô∏è RUNNING SECURITY VULNERABILITY SCAN")
        print("="*60)
        
        try:
            # Import the security scanner module
            from security_scanner import SecurityScanner
            
            scanner = SecurityScanner()
            security_score = scanner.generate_security_report()
            
            # Process vulnerabilities with safe type handling
            # Handle the case where vulnerabilities might be empty or improperly typed
            vulnerabilities = getattr(scanner, 'vulnerabilities', [])
            
            # Count vulnerabilities by severity with proper error handling
            critical_vulns: int = 0
            high_vulns: int = 0
            total_vulns: int = 0
            
            try:
                # Safely iterate through vulnerabilities with type checking
                # Use type casting to handle external module data properly
                if isinstance(vulnerabilities, list):
                    vulns_list = cast(List[Any], vulnerabilities)  # Type cast for safety
                    # Process each vulnerability with comprehensive type validation
                    for vuln in vulns_list:
                        # Ensure each vulnerability is a dictionary with required fields
                        # This handles cases where external modules may return inconsistent data
                        if isinstance(vuln, dict) and 'severity' in vuln:
                            # Safe string conversion with error handling
                            try:
                                # Note: Type checker warns about unknown types from external modules
                                # This is expected behavior when interfacing with dynamically typed modules
                                severity_value = vuln.get('severity', '')  # type: ignore
                                severity = str(severity_value).upper()  # type: ignore  # Normalize severity casing
                                if severity == 'CRITICAL':
                                    critical_vulns += 1
                                elif severity == 'HIGH':
                                    high_vulns += 1
                            except (AttributeError, TypeError) as severity_error:
                                print(f"‚ö†Ô∏è Invalid severity data in vulnerability: {severity_error}")
                    # Calculate total vulnerabilities with type-safe length operation
                    total_vulns = len(vulns_list) if vulns_list else 0
                else:
                    print("‚ö†Ô∏è Vulnerabilities data is not in expected format")
                    print(f"   Expected: list, Got: {type(vulnerabilities)}")
            except Exception as vuln_error:
                print(f"‚ö†Ô∏è Error processing vulnerabilities: {str(vuln_error)}")
                print("   This may indicate an issue with the security scanner module")
            
            # Store results with explicit type annotations
            self.test_results['security_scan'] = {
                'security_score': security_score,
                'total_vulnerabilities': total_vulns,
                'critical_vulnerabilities': critical_vulns,
                'high_vulnerabilities': high_vulns,
                'status': 'PASS' if security_score >= 70 else 'FAIL'
            }
            
            print(f"\nüèÜ Security Score: {security_score}/100")
            print(f"üîç Total Vulnerabilities Found: {total_vulns}")
            print(f"‚ö†Ô∏è Critical: {critical_vulns}, High: {high_vulns}")
            
            return security_score >= 70
            
        except Exception as e:
            print(f"‚ùå Error running security scan: {str(e)}")
            # Store error information with proper typing
            self.test_results['security_scan'] = {
                'status': 'ERROR', 
                'error': str(e),
                'security_score': 0,
                'total_vulnerabilities': 0,
                'critical_vulnerabilities': 0,
                'high_vulnerabilities': 0
            }
            return False

    def run_edge_case_tests(self) -> bool:
        """
        Run edge case testing suite.
        Edge case testing includes:
        - Input validation with extreme values (very long strings, special characters)
        - Boundary value testing (minimum/maximum lengths, limits)
        - Malformed data injection attempts
        - Unicode and encoding edge cases
        - Rate limiting and resource exhaustion scenarios
        - Concurrent access edge cases
        """
        print("\n" + "="*60)
        print("üß™ RUNNING EDGE CASE TESTS")
        print("="*60)
        
        try:
            # Import the edge case testing suite
            from edge_case_tester import EdgeCaseTestSuite
            
            tester = EdgeCaseTestSuite()
            tester.run_all_edge_case_tests()
            
            # Extract test results with proper type handling
            passed: int = tester.edge_cases_passed
            failed: int = tester.edge_cases_failed
            total: int = passed + failed
            
            # Store results with explicit type annotations
            self.test_results['edge_case_tests'] = {
                'passed': passed,
                'failed': failed,
                'total': total,
                'success_rate': (passed/total*100) if total > 0 else 0.0,
                'status': 'PASS' if failed <= (total * 0.2) else 'FAIL'  # Allow 20% failure rate for edge cases
            }
            
            print(f"\nüß™ Edge Case Tests Summary: {passed}/{total} passed")
            return failed <= (total * 0.2)
            
        except Exception as e:
            print(f"‚ùå Error running edge case tests: {str(e)}")
            # Store error information with proper typing
            self.test_results['edge_case_tests'] = {
                'status': 'ERROR', 
                'error': str(e),
                'passed': 0,
                'failed': 0,
                'total': 0,
                'success_rate': 0.0
            }
            return False

    def run_system_diagnostics(self) -> bool:
        """
        Run system debugging and diagnostics - Deep System Health Analysis
        System Diagnostic Areas:
        - Database connectivity and performance analysis
        - Memory usage and resource utilization
        - Configuration validation and consistency checks
        - File system permissions and access validation
        - Network connectivity and API endpoint health
        - Error log analysis and pattern detection
        - System dependency verification
        - Performance benchmarking and bottleneck identification
        """
        print("\n" + "="*60)
        print("üîß RUNNING SYSTEM DIAGNOSTICS")
        print("="*60)
        
        try:
            # Import the system debugging module
            from system_debugger import SystemDebugger
            
            debugger = SystemDebugger()
            debugger.generate_debug_report()
            
            # Process issues found with safe type handling
            # Handle the case where issues might be empty or improperly typed
            issues_found = getattr(debugger, 'issues_found', [])
            recommendations = getattr(debugger, 'recommendations', [])
            
            # Count issues by severity with proper error handling
            critical_issues: int = 0
            total_issues: int = 0
            recommendation_count: int = 0
            
            try:
                # Safely process issues with type checking and casting
                if isinstance(issues_found, list):
                    issues_list = cast(List[Any], issues_found)  # Type cast for safety
                    # Process each issue with comprehensive validation
                    for issue in issues_list:
                        # Ensure each issue is a dictionary with required fields
                        # This handles cases where external modules may return inconsistent data
                        if isinstance(issue, dict) and 'severity' in issue:
                            # Safe string conversion with error handling
                            try:
                                # Note: Type checker warns about unknown types from external modules
                                # This is expected behavior when interfacing with dynamically typed modules
                                severity_value = issue.get('severity', '')  # type: ignore
                                severity = str(severity_value).upper()  # type: ignore  # Normalize severity
                                if severity == 'CRITICAL':
                                    critical_issues += 1
                            except (AttributeError, TypeError) as severity_error:
                                print(f"‚ö†Ô∏è Invalid severity data in issue: {severity_error}")
                    # Calculate total issues with type-safe length operation
                    total_issues = len(issues_list) if issues_list else 0
                else:
                    print("‚ö†Ô∏è Issues data is not in expected format")
                    print(f"   Expected: list, Got: {type(issues_found)}")
                    
                # Safely count recommendations with type casting
                if isinstance(recommendations, list):
                    rec_list = cast(List[Any], recommendations)  # Type cast for safety
                    recommendation_count = len(rec_list) if rec_list else 0
                else:
                    print("‚ö†Ô∏è Recommendations data is not in expected format")
                    print(f"   Expected: list, Got: {type(recommendations)}")
                    
            except Exception as issue_error:
                print(f"‚ö†Ô∏è Error processing diagnostic results: {str(issue_error)}")
                print("   This may indicate an issue with the system debugger module")
            
            # Store results with explicit type annotations
            self.test_results['system_debug'] = {
                'total_issues': total_issues,
                'critical_issues': critical_issues,
                'recommendations': recommendation_count,
                'status': 'PASS' if critical_issues == 0 else 'FAIL'
            }
            
            print(f"\nüîß System Diagnostics Summary: {critical_issues} critical issues found")
            print(f"üîç Total Issues: {total_issues}, Recommendations: {recommendation_count}")
            
            return critical_issues == 0
            
        except Exception as e:
            print(f"‚ùå Error running system diagnostics: {str(e)}")
            # Store error information with proper typing
            self.test_results['system_debug'] = {
                'status': 'ERROR', 
                'error': str(e),
                'total_issues': 0,
                'critical_issues': 0,
                'recommendations': 0
            }
            return False

    def run_existing_test_scripts(self) -> Dict[str, str]:
        """
        Run existing test scripts in the project this includes scripts that are used for preliminary checking of database of admin test, like:
        - test_admin.py: Admin functionality validation and testing
        - check_db.py: Database health check and connectivity validation
        - check_db_content.py: Database content inspection and integrity 
        """
        print("\n" + "="*60)
        print("üîç RUNNING EXISTING TEST SCRIPTS")
        print("="*60)
        
        # Define the specific test scripts to execute with descriptions
        existing_scripts = [
            ('test_admin.py', 'Admin Functionality Test'),
            ('check_db.py', 'Database Health Check'),
            ('check_db_content.py', 'Database Content Inspection')
        ]
        
        # Initialize results dictionary with proper typing
        results: Dict[str, str] = {}
        
        # Import required modules for script execution
        import os          # For file system operations and path checking
        import subprocess  # For executing external Python scripts
        import sys         # For accessing current Python interpreter path
        
        for script, description in existing_scripts:
            # Check if the script file exists before attempting to run it
            # This prevents FileNotFoundError and provides graceful handling
            if os.path.exists(script):
                print(f"\nüìã Running {description}...")
                try:
                    # Execute the script with comprehensive error handling
                    # Uses the same Python interpreter that's running this script
                    # to ensure consistent environment and dependencies
                    result = subprocess.run(
                        [sys.executable, script],  # Command: python script_name.py
                        capture_output=True,       # Capture both stdout and stderr
                        text=True,                # Return string output instead of bytes
                        timeout=30                # 30 second timeout per script
                    )
                    
                    # Analyze execution results based on return code
                    # Return code 0 indicates successful execution
                    if result.returncode == 0:
                        print(f"‚úÖ {description} completed successfully")
                        results[script] = 'PASS'
                    else:
                        print(f"‚ö†Ô∏è {description} completed with warnings")
                        # Show first 200 characters of output for context
                        # This helps identify issues without overwhelming the console
                        if result.stdout:
                            print(f"Output: {result.stdout[:200]}")
                        if result.stderr:
                            print(f"Errors: {result.stderr[:200]}")
                        results[script] = 'WARN'
                        
                except subprocess.TimeoutExpired:
                    # Handle scripts that take too long to execute
                    # This prevents the master test suite from hanging indefinitely
                    print(f"‚è∞ {description} timed out (exceeded 30 seconds)")
                    results[script] = 'TIMEOUT'
                    
                except Exception as e:
                    # Handle any other unexpected errors during script execution
                    # This includes permission errors, import errors, syntax errors, etc.
                    print(f"‚ùå Error running {description}: {str(e)}")
                    results[script] = 'ERROR'
            else:
                # Handle missing script files gracefully
                # This allows the test suite to continue even if some scripts are not present
                print(f"‚è≠Ô∏è {script} not found, skipping...")
                results[script] = 'SKIP'
        
        # Generate and display execution summary statistics
        passed_count = sum(1 for status in results.values() if status == 'PASS')
        total_count = len(existing_scripts)
        
        print(f"\nüìä Existing Scripts Summary: {passed_count}/{total_count} scripts passed")
        
        # Display detailed status for each script with appropriate emojis
        for script, status in results.items():
            status_emoji = {
                'PASS': '‚úÖ',     # Script executed successfully
                'WARN': '‚ö†Ô∏è',     # Script executed but with warnings
                'TIMEOUT': '‚è∞',  # Script exceeded time limit
                'ERROR': '‚ùå',    # Script failed with error
                'SKIP': '‚è≠Ô∏è'      # Script not found, skipped
            }.get(status, '‚ùì')   # Unknown status (shouldn't happen)
            print(f"   {status_emoji} {script}: {status}")
        
        return results

    def generate_final_report(self) -> str:
        """
        Generate comprehensive final report - Master Test Analysis and Documentation
        
        Process:
        1. Calculate total execution time and performance metrics
        2. Analyze results from all test categories
        3. Determine overall system status based on test outcomes
        4. Generate detailed breakdown of each test category
        5. Provide actionable recommendations based on findings
        6. Save comprehensive report to persistent storage
        """
        # Calculate total execution time for performance tracking
        end_time = time.time()
        duration = end_time - self.start_time
        
        print("\n" + "="*80)
        print("üìä FINAL TESTING AND DEBUGGING REPORT")
        print("="*80)
        
        print(f"üïê Total Testing Duration: {duration:.2f} seconds")
        print(f"üìÖ Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # Summary of all test results
        test_categories = [
            ('Comprehensive Tests', self.test_results['comprehensive_tests']),
            ('Security Scan', self.test_results['security_scan']),
            ('Edge Case Tests', self.test_results['edge_case_tests']),
            ('System Diagnostics', self.test_results['system_debug'])
        ]
        
        overall_status = "PASS"
        
        for category, results in test_categories:
            if results:
                status = results.get('status', 'UNKNOWN')
                if status == 'PASS':
                    print(f"‚úÖ {category}: PASSED")
                elif status == 'FAIL':
                    print(f"‚ùå {category}: FAILED")
                    overall_status = "FAIL"
                elif status == 'ERROR':
                    print(f"üî• {category}: ERROR")
                    overall_status = "FAIL"
                else:
                    print(f"‚ùì {category}: {status}")
            else:
                print(f"‚è≠Ô∏è {category}: NOT RUN")
        
        print(f"\nüèÜ OVERALL STATUS: {overall_status}")
        
        # Detailed breakdown
        print("\nüìã DETAILED BREAKDOWN:")
        
        if self.test_results['comprehensive_tests']:
            ct = self.test_results['comprehensive_tests']
            if ct.get('total'):
                print(f"  üöÄ End-to-End Tests: {ct['passed']}/{ct['total']} passed ({ct['success_rate']:.1f}%)")
        
        if self.test_results['security_scan']:
            ss = self.test_results['security_scan']
            if ss.get('security_score'):
                print(f"  üõ°Ô∏è Security Score: {ss['security_score']}/100")
                if ss.get('total_vulnerabilities'):
                    print(f"     Total Vulnerabilities: {ss['total_vulnerabilities']}")
        
        if self.test_results['edge_case_tests']:
            ec = self.test_results['edge_case_tests']
            if ec.get('total'):
                print(f"  üß™ Edge Case Tests: {ec['passed']}/{ec['total']} passed ({ec['success_rate']:.1f}%)")
        
        if self.test_results['system_debug']:
            sd = self.test_results['system_debug']
            if sd.get('total_issues') is not None:
                print(f"  üîß System Issues: {sd['critical_issues']} critical, {sd['total_issues']} total")
        
        # Recommendations
        print("\nüí° RECOMMENDATIONS:")
        
        if overall_status == "PASS":
            print("  ‚úÖ All tests passed! System appears to be functioning correctly.")
            print("  üîÑ Continue regular testing and monitoring.")
            print("  üìà Consider implementing additional security measures for production.")
        else:
            print("  üîß Address failed tests and critical issues immediately.")
            print("  üîç Review detailed test reports for specific recommendations.")
            print("  ‚úÖ Re-run tests after implementing fixes.")
        
        # Save comprehensive report
        self.save_master_report(duration, overall_status)
        
        return overall_status

    def save_master_report(self, duration: float, overall_status: str) -> None:
        """
        Save master test report to file
        
        Report Contents:
        - Execution timestamp and duration
        - Overall test status and summary
        - Detailed results from all test suites
        - Individual test metrics and pass/fail rates
        - Security scan findings and scores
        - System diagnostic results and recommendations
        - Performance metrics and resource utilization
        """
        try:
            # Compile comprehensive report data with proper typing
            report_data: Dict[str, Any] = {
                'timestamp': datetime.now().isoformat(),
                'duration': duration,
                'overall_status': overall_status,
                'test_results': self.test_results,
                'summary': {
                    'comprehensive_tests_passed': self.test_results['comprehensive_tests'].get('status') == 'PASS' if self.test_results['comprehensive_tests'] else False,
                    'security_scan_passed': self.test_results['security_scan'].get('status') == 'PASS' if self.test_results['security_scan'] else False,
                    'edge_case_tests_passed': self.test_results['edge_case_tests'].get('status') == 'PASS' if self.test_results['edge_case_tests'] else False,
                    'system_diagnostics_passed': self.test_results['system_debug'].get('status') == 'PASS' if self.test_results['system_debug'] else False
                },
                'recommendations': self._generate_recommendations(overall_status),
                'system_info': {
                    'python_version': sys.version,
                    'test_framework_version': '1.0.0',
                    'execution_environment': 'development'
                }
            }
            
            # Save report to JSON file with proper formatting
            with open('master_test_report.json', 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nüìÅ Master test report saved to: master_test_report.json")
            print(f"üìä Report contains {len(self.test_results)} test suite results")
            
        except Exception as e:
            print(f"‚ùå Failed to save master report: {str(e)}")
            # Attempt to save a minimal report for troubleshooting
            try:
                minimal_report = {
                    'timestamp': datetime.now().isoformat(),
                    'status': 'ERROR',
                    'error': str(e),
                    'partial_results': str(self.test_results)
                }
                with open('master_test_report_error.json', 'w') as f:
                    json.dump(minimal_report, f, indent=2)
                print("üìÅ Minimal error report saved to: master_test_report_error.json")
            except:
                print("‚ùå Unable to save any report data")

    def _generate_recommendations(self, overall_status: str) -> List[str]:
        """
        Generate actionable recommendations based on test results
        
        Args:
            overall_status (str): Overall test status
            
        Returns:
            List[str]: List of specific recommendations for system improvement
        """
        recommendations: List[str] = []
        
        if overall_status == "PASS":
            recommendations.extend([
                "All tests passed successfully - system is ready for production",
                "Continue regular testing and monitoring",
                "Consider implementing additional security measures for production",
                "Review and update security policies regularly",
                "Monitor system performance in production environment"
            ])
        else:
            recommendations.extend([
                "Address failed tests and critical issues immediately",
                "Review detailed test reports for specific recommendations",
                "Re-run tests after implementing fixes",
                "Consider additional security hardening measures",
                "Implement monitoring and alerting for production deployment"
            ])
            
        # Add specific recommendations based on test results with safe access
        security_scan_result = self.test_results.get('security_scan')
        if security_scan_result and security_scan_result.get('status') == 'FAIL':
            recommendations.append("Critical: Address security vulnerabilities before deployment")
            
        system_debug_result = self.test_results.get('system_debug')
        if system_debug_result and system_debug_result.get('critical_issues', 0) > 0:
            recommendations.append("Critical: Resolve system diagnostic issues before production")
            
        return recommendations

    def run_all_tests(self) -> str:
        """
        Run all testing and debugging procedures
        
        Execution Sequence:
        1. Server connectivity check (prerequisite validation)
        2. System diagnostics (identify infrastructure issues)
        3. Security vulnerability scanning (assess security posture)
        4. Comprehensive end-to-end testing (validate core functionality)
        5. Edge case and boundary testing (stress test robustness)
        6. Legacy script integration (ensure backward compatibility)
        7. Final report generation and documentation
        """
        print("üéØ STARTING MASTER TESTING AND DEBUGGING SUITE")
        print("=" * 80)
        print("Day 8: Comprehensive System Testing and Debugging")
        print("=" * 80)
        
        # Check server status first
        if not self.run_server_check():
            print("\n‚ö†Ô∏è WARNING: Flask server is not running. Some tests may fail.")
            print("Start the server with 'python app.py' for complete testing.\n")
        
        # Run all test suites in logical sequence
        print("\nüîÑ Running all test suites...")
        
        # 1. System Diagnostics (run first to identify system issues)
        print("\nüìã Phase 1: System Infrastructure Validation")
        self.run_system_diagnostics()
        
        # 2. Security Scan (assess security posture)
        print("\nüìã Phase 2: Security Assessment")
        self.run_security_scan()
        
        # 3. Comprehensive End-to-End Tests (core functionality)
        print("\nüìã Phase 3: Core Functionality Testing")
        self.run_comprehensive_tests()
        
        # 4. Edge Case Tests (stress testing)
        print("\nüìã Phase 4: Edge Case and Boundary Testing")
        self.run_edge_case_tests()
        
        # 5. Existing Test Scripts (backward compatibility)
        print("\nüìã Phase 5: Legacy Script Integration")
        legacy_results = self.run_existing_test_scripts()
        
        # Store legacy results for report generation
        self.test_results['legacy_scripts'] = {
            'results': legacy_results,
            'status': 'PASS' if all(status in ['PASS', 'SKIP'] for status in legacy_results.values()) else 'PARTIAL'
        }
        
        # Generate final comprehensive report
        print("\nüìã Phase 6: Report Generation and Analysis")
        overall_status = self.generate_final_report()
        
        # Final status message
        if overall_status == "PASS":
            print("\nüéâ CONGRATULATIONS! All tests passed successfully!")
            print("Your user authentication system is ready for production use.")
            print("üìÑ Review the master_test_report.json for detailed analysis.")
        else:
            print(f"\n‚ö†Ô∏è Testing completed with status: {overall_status}")
            print("üîß Review failed tests and implement recommended fixes.")
            print("üìÑ Check master_test_report.json for specific recommendations.")
            
        return overall_status


# Script execution entry point
if __name__ == "__main__":
    """
    The script will:
    1. Initialize the master test orchestrator
    2. Execute all test suites in the proper sequence
    3. Generate comprehensive reports
    4. Provide actionable feedback and recommendations
    """
    
    print("üöÄ ValTec User Authentication System - Master Test Suite")
    print("=" * 80)
    print("Initializing comprehensive testing and debugging workflow...")
    print("=" * 80)
    
    try:
        # Initialize and run the master test suite
        master_tester = MasterTestSuite()
        final_status = master_tester.run_all_tests()
        
        # Exit with appropriate code for CI/CD integration
        exit_code = 0 if final_status == "PASS" else 1
        
        print(f"\nüèÅ Master Test Suite completed with status: {final_status}")
        print(f"üìä Exiting with code: {exit_code}")
        
        # Provide next steps guidance
        if final_status == "PASS":
            print("\n‚úÖ Next Steps:")
            print("   1. Deploy to production environment")
            print("   2. Set up monitoring and alerting")
            print("   3. Schedule regular security scans")
            print("   4. Review and update documentation")
        else:
            print("\nüîß Next Steps:")
            print("   1. Review detailed test reports")
            print("   2. Fix identified issues and vulnerabilities")
            print("   3. Re-run this test suite")
            print("   4. Validate fixes before deployment")
        
        # Exit with appropriate code for automation integration
        import sys
        sys.exit(exit_code)
        
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è Test suite interrupted by user")
        print("üìÑ Partial results may be available in test report files")
        import sys
        sys.exit(2)
        
    except Exception as e:
        print(f"\nüí• Unexpected error during test execution: {str(e)}")
        print("üìû Please review the error and try again")
        print("üêõ If the error persists, check system dependencies and configuration")
        import sys
        sys.exit(3)

# This is the end of the master_test_suite.py script.